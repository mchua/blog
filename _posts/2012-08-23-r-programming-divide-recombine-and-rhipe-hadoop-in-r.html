---
layout: post
title: 'R Programming: Divide & Recombine, and RHIPE (Hadoop in R)'
date: 2012-08-23 16:27:02.000000000 -04:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Didn't fit anywhere else
tags:
- rprogramming
meta:
  _edit_last: '2'
  _stcr@_jerzywieczorek@gmail.com: 2012-08-23 16:41:27|Y
  aktt_tweeted: '1'
  _stcr@_mark.hoemmen@gmail.com: 2012-08-23 17:23:12|Y
author:
  login: mchua
  email: mel@melchua.com
  display_name: Mel
  first_name: Mel
  last_name: Chua
---
<p>Today's topic is Divide and Recombine in R.</p>
<p>Divide and Recombine breaks things into 3 stages: D(ivide) operations that break the giant dataset into subsets, W(ithin) operations that analyze each subset, and B(etween) operations that put them back together.</p>
<p>This means you're doing a bunch of stuff with parallelism, probably using something for parallel computation like Hadoop. One useful library for this is <a href="http://rhipe.org">RHIPE</a>, a go-between for R and Hadoop; it lets you work with Hadoop by programming entirely in R; Hadoop does D operations, R does the W and B operations. Shiny.</p>
<p>When you're splitting large datasets into small ones for D&amp;R, there's a tradeoff; if you make your subsets large, your processors are chugging away on giant data subsets, which takes a long time and defeats the purpose of trying to split them into smaller bits in the first place. But if you make your subsets small, there are a lot of them, and it takes a long time to put them back together. So you want to get somewhere in the middle, where you have subsets that are small enough to compute efficiently, but few enough in number that they can be recombined efficiently as well.</p>
<p>This stuff matters because we've got people on this campus that crunch datasets that take 3 weeks to process. This means we have ridiculously awesome Linux clusters available on campus. (For instance, one of them is 5 machines, each with 2x Six Core Intel Xeon 2.4GHz processors and 64GB memory, 12x 2TB 7.2k RPM SATA disks in 3 RAID 10s each with their own independent channel out to processors... those are the R/Hadoop ones; the 2 R/RHIPE machines have 128GB memory and <em>16</em> cores, and... I don't even know what I would <em>do</em> with that sort of firepower, good grief. and that's just one of the clusters.)</p>
<p>One of the options for this class (which is breaking into multiple tracks based on experience with R) is to play with RHIPE and Hadoop. Prof. Cleveland says that folks who want to dive into RHIPE need to have a bunch of R experience. I have none. I should probably not do this, but... but... I am tempted, because -- I mean, it's a cool new FOSS project! Let me find out what the other tracks are, and then I'll decide.</p>
