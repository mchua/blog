---
layout: post
title: Sign through your cell phone
date: 2007-02-15 01:08:00.000000000 -05:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Didn't fit anywhere else
tags: []
meta:
  blogger_blog: mellory.blogspot.com
  blogger_permalink: "/2007/02/sign-through-your-cell-phone.html"
author:
  login: mchua
  email: mel@melchua.com
  display_name: Mel
  first_name: Mel
  last_name: Chua
---
<p>If you've got a cell phone but communicate through sign language (or rather, you're <a href="http://www.engadget.com/2007/02/14/detection-algorithms-to-enable-sign-language-on-the-go/">no longer out of luck</a>. Researchers at the Univ. of Washington are working on a project called <a href="http://www.cs.washington.edu/research/MobileASL/index.html">MobileASL</a> that uses a nifty blend of image recognition and compression to enable recognizable signs to be transmitted through normal cell phones (...with cameras and video capabilities, that is).</p>
<p>The reason you can't do this outright is that the bandwidth is way too low to allow the high-res, high-speed data you need to pick out facial expressions and subtle finger movements. It's like reading a book from 10 feet away with blurry glasses. You might be able to squint some things out, but can't really get a handle on the information, and there's all this visual data (the things around the book that fall within your field of vision) that isn't useful at all, but is just sitting there taking up space on your retina.</p>
<p>What MobileASL does is very smart; it recognizes the important parts of the video (your hands) and prioritizes encoding and transmission so that <span style="font-style: italic;">just those parts</span> come through faster, with better quality. Selective focusing. Bandwidth goes way down. Comprehensibility goes up (presumably). No fancy algorithms to recognize individual signs is needed (that's <span style="font-style: italic;">really</span> hard - think speech recognition, only you're using pictures instead of sound), just enough edge detection to tell you where the hands are.</p>
<p>I wonder if they could do the same for lipreading, with a slight twist. One of the reasons videoconferencing is so tough for me is that the frame rate and resolution (due to bandwidth limitations) aren't high enough for me to read lips, but if you did selective image/facial recognition and compression so that just the lips came through at higher speeds with more fidelity (and the rest of the video came through, but was very low-res)... Hmm.</p>
<p>It bothers me slightly that I'm becoming more interested in technologies for the deaf and hard-of-hearing (I know some people take offense at the latter term, but it's the most common usage as well as what I call myself, so until such time as I find a better one, I'll use it). But the stuff they're doing with technology there is just so darn cool, I'm letting myself track it down anyway.</p>
